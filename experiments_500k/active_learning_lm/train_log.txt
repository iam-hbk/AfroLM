2023-11-07 18:17:19,807 Active Learning Step 1
2023-11-07 18:17:19,807 Experiment Output Path: experiments_500k/active_learning_lm
2023-11-07 18:17:19,807 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'tokenizer_250k', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 256, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 10, 'intermediate_size': 3072, 'hidden_act': 'gelu', 'position_embedding_type': 'absolute', 'hidden_dropout_prob': 0.1, 'initializer_range': 0.02}, 'training': {'gradient_accumulation_steps': 8, 'ignore_data_skip': False, 'overwrite_output_dir': False, 'seed': 1234, 'max_steps': 500000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 200000, 'save_total_limit': 1, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments_500k/active_learning_lm'}, 'data': {'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2023-11-07 18:17:19,807 Training from scratch...
2023-11-07 18:17:20,133 Building model...
2023-11-07 18:17:32,218 Model built with num parameters: 263922834
2023-11-07 18:17:32,218 Building datasets...
2023-11-07 18:17:32,218 Building train dataset from data/train/...
2023-11-07 18:21:23,259 No. of training sentences: 926180
2023-11-07 18:21:23,259 Building evaluation dataset from data/eval/all_eval.txt...
2023-11-07 18:22:20,408 No. of evaluation sentences: 230369
2023-11-07 18:22:20,409 Starting Training...
2023-11-07 18:26:53,473 Active Learning Step 1
2023-11-07 18:26:53,474 Experiment Output Path: experiments_500k/active_learning_lm
2023-11-07 18:26:53,474 Training will be done with this configuration: 
 {'model': {'tokenizer_path': 'tokenizer_250k', 'layer_norm_eps': 1e-05, 'output_past': True, 'type_vocab_size': 1, 'max_length': 256, 'hidden_size': 768, 'num_attention_heads': 6, 'num_hidden_layers': 10, 'intermediate_size': 3072, 'hidden_act': 'gelu', 'position_embedding_type': 'absolute', 'hidden_dropout_prob': 0.1, 'initializer_range': 0.02}, 'training': {'gradient_accumulation_steps': 8, 'ignore_data_skip': False, 'overwrite_output_dir': False, 'seed': 1234, 'max_steps': 500000, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'dataloader_num_workers': 6, 'fp16': True, 'save_steps': 200000, 'save_total_limit': 1, 'learning_rate': 0.0001, 'warmup_steps': 40000, 'output_dir': 'experiments_500k/active_learning_lm'}, 'data': {'train': 'data/train/', 'eval': {'all': 'data/eval/all_eval.txt', 'per_lang': 'data/eval/'}}} 
2023-11-07 18:26:53,474 Training from scratch...
2023-11-07 18:26:53,811 Building model...
2023-11-07 18:27:05,820 Model built with num parameters: 263922834
2023-11-07 18:27:05,821 Building datasets...
2023-11-07 18:27:05,821 Building train dataset from data/train/...
2023-11-07 18:31:03,718 No. of training sentences: 926180
2023-11-07 18:31:03,719 Building evaluation dataset from data/eval/all_eval.txt...
2023-11-07 18:32:01,288 No. of evaluation sentences: 230369
2023-11-07 18:32:01,289 Starting Training...
